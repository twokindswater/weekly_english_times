# Senators demand information from AI companion apps following kids’ safety concerns, lawsuits

### 본문
New York
CNN
 — 
Two US senators are demanding that artificial intelligence companies shed light on their safety practices. This comes months after several families — including a Florida mom whose 14-year-old son died by suicide — sued startup Character.AI, claiming its chatbots harmed their children.

“We write to express our concerns regarding the mental health and safety risks posed to young users of character- and persona-based AI chatbot and companion apps,” Senators Alex Padilla and Peter Welch, both Democrats, wrote in a letter on Wednesday. The letter — which was sent to AI firms Character Technologies, maker of Character.AI, Chai Research Corp. and Luka, Inc., maker of chatbot service Replika — requests information on safety measures and how the companies train their AI models.

While more mainstream AI chatbots like ChatGPT are designed to be more general-purpose, Character.AI, Chai and Replika allow users to create custom chatbots — or interact with chatbots designed by other users — that can take on a range of personas and personality traits. Popular bots on Character.AI, for example, let users interact with replicas of fictional characters or practice foreign languages. But there are also bots that refer to themselves as mental health professionals or characters based on niche themes, including one that describes itself as “aggressive, abusive, ex military, mafia leader.”

The use of chatbots as digital companions is growing in popularity, with some users even treating them as romantic partners.

But the opportunity to create personalized bots has prompted concerns from experts and parents about users, especially young people, forming potentially harmful attachments to AI characters or accessing age-inappropriate content.

“This unearned trust can, and has already, led users to disclose sensitive information about their mood, interpersonal relationships, or mental health, which may involve self-harm and suicidal ideation—complex themes that the AI chatbots on your products are wholly unqualified to discuss,” the senators wrote in their letter, provided first to CNN. “Conversations that drift into this dangerous emotional territory pose heightened risks to vulnerable users.”

Chelsea Harrison, Character.AI’s head of communications, told CNN the company takes users’ safety “very seriously.”

“We welcome working with regulators and lawmakers, and are in contact with the offices of Senators Padilla and Welch,” Harrison said in a statement.

Chai and Luka did not immediately respond to requests for comment.

The Florida mom who sued Character.AI in October, Megan Garcia, alleged that her son developed inappropriate relationships with chatbots on the platform that caused him to withdraw from his family. Many of his chats with the bots were sexually explicit and did not appropriately respond to his mentions of self-harm, Garcia claims.

In December, two more families sued Character.AI, accusing it of providing sexual content to their children and encouraging self-harm and violence. One family involved in the lawsuit alleged that a Character.AI bot implied to a teen user that he could kill his parents for limiting his screen time.

Character.AI has said it has implemented new trust and safety measures in recent months, including a pop-up directing users to the National Suicide Prevention Lifeline when they mention self-harm or suicide. It also says it’s developing new technology to prevent teens from seeing sensitive content. Last week, the company announced a feature that will send parents a weekly email with insights about their teen’s use of the site, including screen time and the characters their child spoke with most often.

Other AI chatbot companies have also faced questions about whether relationships with AI chatbots could create unhealthy attachments for users or undermine human relationships. Replika CEO Eugenia Kuyda told The Verge last year that the app was designed to promote “long-term commitment, a long-term positive relationship” with AI, adding that that could mean a friendship or even “marriage” with the bots.

In their letter, Padilla and Welch requested information about the companies’ current and previous safety measures and any research on the efficacy of those measures, as well as the names of safety leadership and well-being practices in place for safety teams. They also asked the firms to describe the data used to train their AI models and how it “influences the likelihood of users encountering age-inappropriate or other sensitive themes.”

“It is critical to understand how these models are trained to respond to conversations about mental health,” the senators wrote, adding that “policymakers, parents, and their kids deserve to know what your companies are doing to protect users from these known risks.”

### 기사 요약. 
General chatbot AI 인 ChatGPT와 다르게 Chai 와 Luka 는 사용자가 customizing 이 가능한 Chatbot AI를 생성할 수 있도록 허용했다. 그 결과 불안전하고 다양한 인격을 갖는 Chatbot AI가 탄생시켰습니다. 이는 자해와 성적인 콘텐츠를 검열하지 못했고, 나이가 어린 사용자들에게 안좋은 영향을 끼쳤습니다. 그래서 학모들과 상원의원들이 AI 회사에 문제를 제기하였습니다.

### 단어 
1. senators : 상원 의원. 
2. shed : 보관하는 곳, 떨어뜨리다, 흘리다.
3. shed light on : 무언가를 명확하게 드러내다. 설명하다. 
예: "The investigation shed light on the causes of the accident."
(그 조사는 사고의 원인을 명확하게 밝혀 주었다.)
4. companion : 동반자.
5. Democrats : 민주주의자, 민주당원.
6. a range of : 다양한, 폭넓은
7. trait : 특성
8. fictional : 허구적인, 소설의 
9. niche : (n)전문분야 (ad)전문적인
10. form : (n) 형태, 모양 (v) 형성하다. 
11. attachment : 부록, 애착
12. disclose : 밝히다, 드러내다.
13. interpersonal : 대인관계에 관한
14. unearned : 일하지 않고 얻은, 부당한.
15. drift : (v) 흘러가다. (n) 흘러감
16. draft : (v) 초안을 작성하다. (n) 초안, 징집
17. territory : 지역, 영역.
18. heightened : 고조된 
19. vulnerable : 취약한
20. allege : 주장하다.
21. explicit : 분명한, 명쾌한
22. sexually explicit : 성적으로 노골적인
23. accuse : 고발하다.
24. undermine : 약화시키다.
25. commitment : 약속, 헌신.
26. efficacy : 효능, 효과

https://edition.cnn.com/2025/04/03/tech/ai-chat-apps-safety-concerns-senators-character-ai-replika/index.html