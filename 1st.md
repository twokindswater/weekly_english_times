# Senators demand information from AI companion apps following kids’ safety concerns, lawsuits

### 본문
New York
CNN
 — 
Two US senators are demanding that artificial intelligence companies shed light on their safety practices. This comes months after several families — including a Florida mom whose 14-year-old son died by suicide — sued startup Character.AI, claiming its chatbots harmed their children.

“We write to express our concerns regarding the mental health and safety risks posed to young users of character- and persona-based AI chatbot and companion apps,” Senators Alex Padilla and Peter Welch, both Democrats, wrote in a letter on Wednesday. The letter — which was sent to AI firms Character Technologies, maker of Character.AI, Chai Research Corp. and Luka, Inc., maker of chatbot service Replika — requests information on safety measures and how the companies train their AI models.

While more mainstream AI chatbots like ChatGPT are designed to be more general-purpose, Character.AI, Chai and Replika allow users to create custom chatbots — or interact with chatbots designed by other users — that can take on a range of personas and personality traits. Popular bots on Character.AI, for example, let users interact with replicas of fictional characters or practice foreign languages. But there are also bots that refer to themselves as mental health professionals or characters based on niche themes, including one that describes itself as “aggressive, abusive, ex military, mafia leader.”

The use of chatbots as digital companions is growing in popularity, with some users even treating them as romantic partners.

But the opportunity to create personalized bots has prompted concerns from experts and parents about users, especially young people, forming potentially harmful attachments to AI characters or accessing age-inappropriate content.

“This unearned trust can, and has already, led users to disclose sensitive information about their mood, interpersonal relationships, or mental health, which may involve self-harm and suicidal ideation—complex themes that the AI chatbots on your products are wholly unqualified to discuss,” the senators wrote in their letter, provided first to CNN. “Conversations that drift into this dangerous emotional territory pose heightened risks to vulnerable users.”

### 기사 요약. 
General chatbot AI 인 ChatGPT와 다르게 Chai 와 Replica 는 사용자가 customizing 이 가능한 Chatbot AI를 생성할 수 있도록 허용했다. 그 결과 다양한 인격을 갖는 Chatbot AI가 탄생하였고, 이는 불안정한 Chatbot AI로 인해 다양한 잠재적 문제를 발생할 우려가 있다. 이는 젊은세대에 특히 취약하다. 

### 단어 
1. senators : 상원 의원. 
2. shed : 보관하는 곳, 떨어뜨리다, 흘리다.
3. shed light on : 무언가를 명확하게 드러내다. 설명하다. 
예: "The investigation shed light on the causes of the accident."
(그 조사는 사고의 원인을 명확하게 밝혀 주었다.)
4. companion : 동반자.
5. Democrats : 민주주의자, 민주당원.
6. a range of : 다양한, 폭넓은
7. trait : 특성
8. fictional : 허구적인, 소설의 
9. niche : (n)전문분야 (ad)전문적인
10. form : (n) 형태, 모양 (v) 형성하다. 
11. attachment : 부록, 애착
12. disclose : 밝히다, 드러내다.
13. interpersonal : 대인관계에 관한
14. unearned : 일하지 않고 얻은, 부당한.
15. drift : (v) 흘러가다. (n) 흘러감
16. draft : (v) 초안을 작성하다. (n) 초안, 징집
17. territory : 지역, 영역.
18. heightened : 고조된 
19. vulnerable : 취약한

https://edition.cnn.com/2025/04/03/tech/ai-chat-apps-safety-concerns-senators-character-ai-replika/index.html